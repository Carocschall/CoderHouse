{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOK4RVOddhQhgqPSGwX8Clq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carocschall/CoderHouse/blob/main/DataScienceIII_CortezSchall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA SCIENCE III: NLP Y DEEP LEARNING APLICADO A LA CIENCIA DE DATOS - Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "XPiusanhkBwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alumna: Carolina Cortez Schall"
      ],
      "metadata": {
        "id": "5DGXRpoblUgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Presentación del proyecto**"
      ],
      "metadata": {
        "id": "BC3puDGClbTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstracto con Motivación y Audiencia\n",
        "\n",
        "Contexto Comercial y Analítico\n",
        "\n",
        "Preguntas/Hipótesis a Resolver\n",
        "\n",
        "Objetivo"
      ],
      "metadata": {
        "id": "dItLUymjliTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lectura de datos**"
      ],
      "metadata": {
        "id": "IbjVHbDdmI-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Librerias necesarias**"
      ],
      "metadata": {
        "id": "ZuQ77pqVmLAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEvGv6o3EOTX",
        "outputId": "97960676-a53c-41f7-b114-0c1c42c26a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.10/dist-packages (6.7.8)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from symspellpy) (0.1.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "! pip install -U symspellpy\n",
        "import nltk # importar natural language toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords') # modulo para descargar stopwords en diferentes idiomas\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import re\n",
        "import string\n",
        "import plotly\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "import time\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud\n",
        "import pickle\n",
        "from symspellpy import SymSpell\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga de datos**"
      ],
      "metadata": {
        "id": "TtZ01TRqQ8oq"
      }
    },
    {
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# El ensayo que analizaremos se encuentra en Medium\n",
        "url = \"https://medium.com/kambrica/a-20-a%C3%B1os-del-innotec-visualizando-las-relaciones-encuentros-y-desencuentros-entre-ciencia-2c3b36bb53f9\"\n",
        "\n",
        "# El código suele traer contenido que no es relevante para análisis. Para esto identificamos frases para identificar el inicio y el fin del contenido relevante\n",
        "frase_inicial = \"En 2004, el IAPG (Instituto Argentino del Petróleo y el Gas)\"\n",
        "frase_final = \"aportando lo mejor que tenemos para construir un mundo mejor.\"\n",
        "\n",
        "# Enviar una solicitud GET para obtener el contenido HTML bruto\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200: #valida si la página carga correctamente\n",
        "    # Parsear el contenido HTML (para que Python lo entienda) usando BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Extraer todo el texto de las etiquetas de párrafo\n",
        "    paragraphs = soup.find_all('p')\n",
        "    article_text = '\\n'.join([para.get_text() for para in paragraphs])#se une todo el texto y separa salto de linea\n",
        "\n",
        "    # Extraer la porción relevante del texto\n",
        "    start_index = article_text.find(frase_inicial)\n",
        "    end_index = article_text.find(frase_final) + len(frase_final)\n",
        "#Si ambas frases (inicial y final) se encuentran en el texto, se extrae el bloque de texto entre esas frases.\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        texto_relevante = article_text[start_index:end_index]\n",
        "        print(texto_relevante)\n",
        "    else:\n",
        "        print(\"No se encontraron las frases especificadas en el artículo.\")\n",
        "else:\n",
        "    print(f\"Fallo al recuperar la página web. Código de estado: {response.status_code}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMv7I-ExWvUk",
        "outputId": "844b1921-4684-49e9-ce5c-4ff0169ba87b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "En 2004, el IAPG (Instituto Argentino del Petróleo y el Gas) y la SPE de Argentina (capítulo local de la Society Of Petroleum Engineers), organizaron el iNNotec: 1er Expo-Congreso de Innovación Tecnológica en Energía y Petroquímica.\n",
            "El Director General del IAPG, Roberto E. Cunningham, decidicó que un congreso de innovación merecía una introducción innovadora: su “Sainete Criollo de Inocencio Ricerca y Empresio Mandattori”.\n",
            "En lugar de un tradicional discurso, desarrolló los temas de la conferencia en una obra de teatro leído, que sería dirigida por Claudio Moreno e interpretada por Páncho Ibáñez y Hugo Bob Quintela, con el apoyo de infografías, animaciones e imágenes situacionales cuyo diseño y desarrollo tuve a cargo.\n",
            "En sus propias palabras:\n",
            "…Sentí la necesidad de redactar algunas pautas, algo así como un manual instructivo a ser seguido por un científico toda vez que le tocara exponer ante un empresario. Así lo hice, pero no quedé conforme, pues veía el texto desarrollado como un mero reglamento a cumplir.Pensé entonces que el mensaje podría ser más convincente y persuasivo, además de entretenido, si lo expresaba a través de un supuesto diálogo entre un investigador y un empresario.Algo así, sin pretender emular, como se da en la obra teatral Copenhague.[…]Y de este modo fue saliendo el diálogo, con agregados, enmiendas y recurrencias. El diálogo que va a continuación, y que la Comisión Organizadora de iNNotec ha decidido utilizar, en un acto innovador, en lugar de la tradicional conferencia inaugural de todo Congreso.\n",
            "A lo largo de varios meses, tuve el honor de trabar con Roberto Cunningham y su mano derecha Susana Borgato para llevar a cabo su visión. En este proyecto, aprendí muchísimo de él y de su experiencia en los desafíos de la innovación y la tecnología en la región.\n",
            "La infografía que diseñé y desarrollé, siguiendo ideas del gran Jef Raskin, mostraba en un solo plano todos los conceptos y relaciones a medida surgían en la obra. El resultado fue un solo lienzo que funcionaba a distintos niveles de zoom — un criterio innovador para ese entonces, años después popularizado por Prezi.\n",
            "Les comparto a continuación un resumen de cómo se iban desarrollando y visualizando los conceptos.\n",
            "La obra comienza en el contexto de una reunión en la que empresarios reciben a investigadores exponiendo sus últimos resultados en el campo de la Investigación, Desarrollo e Innovación.\n",
            "El investigador, Inocencio Ricerca, comienza entusiasmado hablando del proceso que ha desarrollado. El empresario, Empresio Mandattori, lo interrumpe: su interés está en su cliente, no en el desarrollo de productos sin demanda. El investigador salva la diferencia y comienzan a plantearse los aspectos del proceso que hacen al producto, la distinción entre innovación tecnológica vs. curiosidad técnica, diferenciales y desafíos para llevar el producto al mercado, entre otros:\n",
            "Lo que interesa es la función y no el producto como tal.Como diría Panzeri hablando de fútbol: “no hay puestos, hay funciones”.\n",
            "Es así como de una relación a la otra, llegan a las reglas de oro de Jorge Sábato, que marcan la diferencia entre un científico y un tecnólogo:\n",
            "A medida van surgiendo los desafíos de llevar adelante la innovación al mercado, el empresario observa:\n",
            "en la Facultad siempre se habla de procesos, de productos se habla mucho menos y de ventas nunca. Y no es así. Porque con los datos del proceso hay que desarrollar las tres ingenierías, el “paquete” que le dicen……nombre que le puso Jorge Sábato cuando preconizaba que la actitud inteligente del comprador de las Ingenierías era no comprar una caja negra a libro cerrado sino abrir su contenido para no encontrarse con sorpresas desagradables.Y Jorge, tecnológo, futbolero y tanguero genial no encontró mejor analogía para el caso que hablar de “abrir el paquete” en homenaje a Enrique Santos Discépolo:“Me da tristeza el panete, chicato inocente que se la llevó… ¡Cuando desate el paquete y manye que se ensartó!”\n",
            "Los conceptos y relaciones se suceden, hasta que resulta necesario un cambio de escala para tratar las relaciones entre Estado, Empresas e Instituciones científicas:\n",
            "Si [lo que estamos conversando] hubiese sido sobre una idea que los investigadores ya tenían, podría haber cabido juntar algunas empresas para financiar la primera etapa del estudio [Investigación precompetitiva]Ello, en la medida que éste fuera lo suficientemente general como para que pudiera ser explotado luego por distintas empresas. [Inversión de riesgo]Y si retrocediéramos aún un paso más, Uds. podrían haber sometido un listado de ideas para analizar [Selección de ideas de negocio]\n",
            "En ello, se toca el punto: ¿de dónde salen las ideas? ¿De un grupo de científicos, o un grupo de tecnólogos?\n",
            "Tienen que salir de los grupos de ventas. Es la única manera de responder a la demanda. En una empresa, las ideas de desarrollo no salen de la Gerencia de Desarrollo. Salen de la Gerencia de Ventas.\n",
            "De esta manera, aparece el Cliente como entidad en la visualización. Y cobra sentido la infografía: hacia la izquierda, están el cliente y el producto tangible. Y hacia la derecha, el proceso, la investigación, y en definitiva y de forma progresiva, todo aquello que el Cliente no ve: lo que hoy en un mapa de Wardley se presenta de arriba a abajo.\n",
            "En esta visualización, el nivel de abstracción a la derecha es máximo: ya no es la tecnología, ni la Ciencia: son las reflexiones filosóficas de Florencio Escardó y Ernesto Sábato.\n",
            "Surgen la cuestión de la falta de interlocures válidos entre Empresas e Instituciones Científicas, y la visión del Desarrollo: a corto plazo, se busca comprar tecnologías ya probadas, con garantía de funcionamiento y asesoramiento. Se plantea entonces:\n",
            "…el dilema del subdesarrollo: en un país desarrollado, a una empresa que no invierte en innovar le puede costar muy caro; en cambio, en un país subdesarrollado, es a la empresa que invierta en innovar que le puede costar muy caro. Pero al mismo tiempo, si no innovamos, no saldremos…\n",
            "En la charla, surgen las cuestiones legales (ej. patentes), su marcoy el rol del estdo entre las relaciones Ciencia-Empresa:\n",
            "En el caso de los commodities, Ud. tiene un mercado abierto de tecnología. Pone los “verdes” sobre la mesa y listo. Se trata de producciones de grandes volúmenes con muchos fierros. […] Sería absurdo poner en la lista de prioridades nacionales al desarrollo de commodities. Sería carísimo.A su vez, hay otros casos en los que Ud. se encuentra con patentes. O bien va al pie y paga las regalías del caso, o intenta descifrar la patente y encontrar una variante que le permita esquivarla. Hay muchos profesionales expertos en esa materia. Con lo cual, el mismo hecho de patentar implica un riesgo.Por eso queda la tercera variante: en la que el secreto es tal que ni siquiera se lo patenta.\n",
            "Roberto Cunningham plantea la épica del Maracanazo para desbancar la idea del foco en “Planes de Estudios” que ignoran a los docentes.\n",
            "— Ud. ni siquiera había nacido, pero seguramente oyó hablar del Uruguay campeón del mundo en el Maracaná en el 50.\n",
            "– Sí, mi padre tenía un primo uruguayo y cada vez que se juntaban hablaban del Maracanazo. […] Nadie daba un peso por el equipo uruguayo. Se habían conocido en el barco de ida a Río.\n",
            "– Con el empate, Brasil era campeón… y ganaba por 1 a 0… ¿Y su padre y su primo oriental, le hablaban de los jugadores?\n",
            "– Pero cómo no, de Obdulio, de Ghiggia, de Máspoli. ¡Máspoli!\n",
            "— Y de las tácticas. ¿No hablaban de las tácticas?.\n",
            "— No. ¿Qué quiere que dijeran de las tácticas?.\n",
            "— No quiero nada. Pero fíjese que ese equipo, como cualquier otro, pasa a la historia por sus jugadores y no por sus tácticas.Las tácticas son los planes de estudio. Los jugadores son los maestros.Déme el nombre de una universidad que haya pasado a la historia por sus planes de estudio. En cambio, Aristóteles, Francis Bacon, Newton, Maxwell …\n",
            "Al final de la obra, la infografía queda completa, exponiendo y relacionando todos los conceptos desarrollados.\n",
            "Queda brevemente en pantalla, y se apaga lentamente mientras suena la música de cierre.\n",
            "En su genial obra, Roberto logró entrelazar ciencia, tecnología, políticas públicas… y también tango y fútbol. En sus propias palabras:\n",
            "Jorge Alberto Sabato fue un físico que dirigió por muchos años el Departamento de Tecnología de Materiales de la Comisión Nacional de Energía Atómica y es el adalid y mentor de la gestión tecnológica en América latina.Florencio Escardó fue médico pediatra de dilatada atención en el campo de la medicina y, antes que ello, un maestro de juventudes.Dante Enrique Panzeri fue un periodista deportivo que en la década del ’60 revolucionó el mundo del fútbol desde las páginas de la revista El Gráfico.Los tres, ya desaparecidos, constituyen la mejor síntesis y representación de esa trilogía tecnología, tango, fútbol (e innovación) a que aludimos.\n",
            "Con el fallecimiento del Dr. Cunningham en octubre de 2008, la comunidad científica y empresarial perdió a un líder visionario que supo unir mundos.\n",
            "Trabajar junto a él fue una experiencia inolvidable; compartir con ustedes el producto de ese encuentro es una forma de recordarlo, y de valorar todos los aportes aún vigentes de su mirada al desarrollo tecnológico y la innovación.\n",
            "Que su memoria siga inspirándonos a seguir explorando nuevos horizontes y aportando lo mejor que tenemos para construir un mundo mejor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lectura del corpus**"
      ],
      "metadata": {
        "id": "OHrgcMYsmirp"
      }
    },
    {
      "source": [
        "import nltk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# ... (your existing code to fetch and extract text) ...\n",
        "\n",
        "# Download the 'punkt_tab' data package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Apply word tokenization\n",
        "tokenized_palabra = word_tokenize(texto_relevante)\n",
        "print(tokenized_palabra)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-msna_OdOj8",
        "outputId": "3b27b6bb-b38c-4aac-a5cc-c2db4757e55c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', '2004', ',', 'el', 'IAPG', '(', 'Instituto', 'Argentino', 'del', 'Petróleo', 'y', 'el', 'Gas', ')', 'y', 'la', 'SPE', 'de', 'Argentina', '(', 'capítulo', 'local', 'de', 'la', 'Society', 'Of', 'Petroleum', 'Engineers', ')', ',', 'organizaron', 'el', 'iNNotec', ':', '1er', 'Expo-Congreso', 'de', 'Innovación', 'Tecnológica', 'en', 'Energía', 'y', 'Petroquímica', '.', 'El', 'Director', 'General', 'del', 'IAPG', ',', 'Roberto', 'E.', 'Cunningham', ',', 'decidicó', 'que', 'un', 'congreso', 'de', 'innovación', 'merecía', 'una', 'introducción', 'innovadora', ':', 'su', '“', 'Sainete', 'Criollo', 'de', 'Inocencio', 'Ricerca', 'y', 'Empresio', 'Mandattori', '”', '.', 'En', 'lugar', 'de', 'un', 'tradicional', 'discurso', ',', 'desarrolló', 'los', 'temas', 'de', 'la', 'conferencia', 'en', 'una', 'obra', 'de', 'teatro', 'leído', ',', 'que', 'sería', 'dirigida', 'por', 'Claudio', 'Moreno', 'e', 'interpretada', 'por', 'Páncho', 'Ibáñez', 'y', 'Hugo', 'Bob', 'Quintela', ',', 'con', 'el', 'apoyo', 'de', 'infografías', ',', 'animaciones', 'e', 'imágenes', 'situacionales', 'cuyo', 'diseño', 'y', 'desarrollo', 'tuve', 'a', 'cargo', '.', 'En', 'sus', 'propias', 'palabras', ':', '…Sentí', 'la', 'necesidad', 'de', 'redactar', 'algunas', 'pautas', ',', 'algo', 'así', 'como', 'un', 'manual', 'instructivo', 'a', 'ser', 'seguido', 'por', 'un', 'científico', 'toda', 'vez', 'que', 'le', 'tocara', 'exponer', 'ante', 'un', 'empresario', '.', 'Así', 'lo', 'hice', ',', 'pero', 'no', 'quedé', 'conforme', ',', 'pues', 'veía', 'el', 'texto', 'desarrollado', 'como', 'un', 'mero', 'reglamento', 'a', 'cumplir.Pensé', 'entonces', 'que', 'el', 'mensaje', 'podría', 'ser', 'más', 'convincente', 'y', 'persuasivo', ',', 'además', 'de', 'entretenido', ',', 'si', 'lo', 'expresaba', 'a', 'través', 'de', 'un', 'supuesto', 'diálogo', 'entre', 'un', 'investigador', 'y', 'un', 'empresario.Algo', 'así', ',', 'sin', 'pretender', 'emular', ',', 'como', 'se', 'da', 'en', 'la', 'obra', 'teatral', 'Copenhague', '.', '[', '…', ']', 'Y', 'de', 'este', 'modo', 'fue', 'saliendo', 'el', 'diálogo', ',', 'con', 'agregados', ',', 'enmiendas', 'y', 'recurrencias', '.', 'El', 'diálogo', 'que', 'va', 'a', 'continuación', ',', 'y', 'que', 'la', 'Comisión', 'Organizadora', 'de', 'iNNotec', 'ha', 'decidido', 'utilizar', ',', 'en', 'un', 'acto', 'innovador', ',', 'en', 'lugar', 'de', 'la', 'tradicional', 'conferencia', 'inaugural', 'de', 'todo', 'Congreso', '.', 'A', 'lo', 'largo', 'de', 'varios', 'meses', ',', 'tuve', 'el', 'honor', 'de', 'trabar', 'con', 'Roberto', 'Cunningham', 'y', 'su', 'mano', 'derecha', 'Susana', 'Borgato', 'para', 'llevar', 'a', 'cabo', 'su', 'visión', '.', 'En', 'este', 'proyecto', ',', 'aprendí', 'muchísimo', 'de', 'él', 'y', 'de', 'su', 'experiencia', 'en', 'los', 'desafíos', 'de', 'la', 'innovación', 'y', 'la', 'tecnología', 'en', 'la', 'región', '.', 'La', 'infografía', 'que', 'diseñé', 'y', 'desarrollé', ',', 'siguiendo', 'ideas', 'del', 'gran', 'Jef', 'Raskin', ',', 'mostraba', 'en', 'un', 'solo', 'plano', 'todos', 'los', 'conceptos', 'y', 'relaciones', 'a', 'medida', 'surgían', 'en', 'la', 'obra', '.', 'El', 'resultado', 'fue', 'un', 'solo', 'lienzo', 'que', 'funcionaba', 'a', 'distintos', 'niveles', 'de', 'zoom', '—', 'un', 'criterio', 'innovador', 'para', 'ese', 'entonces', ',', 'años', 'después', 'popularizado', 'por', 'Prezi', '.', 'Les', 'comparto', 'a', 'continuación', 'un', 'resumen', 'de', 'cómo', 'se', 'iban', 'desarrollando', 'y', 'visualizando', 'los', 'conceptos', '.', 'La', 'obra', 'comienza', 'en', 'el', 'contexto', 'de', 'una', 'reunión', 'en', 'la', 'que', 'empresarios', 'reciben', 'a', 'investigadores', 'exponiendo', 'sus', 'últimos', 'resultados', 'en', 'el', 'campo', 'de', 'la', 'Investigación', ',', 'Desarrollo', 'e', 'Innovación', '.', 'El', 'investigador', ',', 'Inocencio', 'Ricerca', ',', 'comienza', 'entusiasmado', 'hablando', 'del', 'proceso', 'que', 'ha', 'desarrollado', '.', 'El', 'empresario', ',', 'Empresio', 'Mandattori', ',', 'lo', 'interrumpe', ':', 'su', 'interés', 'está', 'en', 'su', 'cliente', ',', 'no', 'en', 'el', 'desarrollo', 'de', 'productos', 'sin', 'demanda', '.', 'El', 'investigador', 'salva', 'la', 'diferencia', 'y', 'comienzan', 'a', 'plantearse', 'los', 'aspectos', 'del', 'proceso', 'que', 'hacen', 'al', 'producto', ',', 'la', 'distinción', 'entre', 'innovación', 'tecnológica', 'vs.', 'curiosidad', 'técnica', ',', 'diferenciales', 'y', 'desafíos', 'para', 'llevar', 'el', 'producto', 'al', 'mercado', ',', 'entre', 'otros', ':', 'Lo', 'que', 'interesa', 'es', 'la', 'función', 'y', 'no', 'el', 'producto', 'como', 'tal.Como', 'diría', 'Panzeri', 'hablando', 'de', 'fútbol', ':', '“', 'no', 'hay', 'puestos', ',', 'hay', 'funciones', '”', '.', 'Es', 'así', 'como', 'de', 'una', 'relación', 'a', 'la', 'otra', ',', 'llegan', 'a', 'las', 'reglas', 'de', 'oro', 'de', 'Jorge', 'Sábato', ',', 'que', 'marcan', 'la', 'diferencia', 'entre', 'un', 'científico', 'y', 'un', 'tecnólogo', ':', 'A', 'medida', 'van', 'surgiendo', 'los', 'desafíos', 'de', 'llevar', 'adelante', 'la', 'innovación', 'al', 'mercado', ',', 'el', 'empresario', 'observa', ':', 'en', 'la', 'Facultad', 'siempre', 'se', 'habla', 'de', 'procesos', ',', 'de', 'productos', 'se', 'habla', 'mucho', 'menos', 'y', 'de', 'ventas', 'nunca', '.', 'Y', 'no', 'es', 'así', '.', 'Porque', 'con', 'los', 'datos', 'del', 'proceso', 'hay', 'que', 'desarrollar', 'las', 'tres', 'ingenierías', ',', 'el', '“', 'paquete', '”', 'que', 'le', 'dicen……nombre', 'que', 'le', 'puso', 'Jorge', 'Sábato', 'cuando', 'preconizaba', 'que', 'la', 'actitud', 'inteligente', 'del', 'comprador', 'de', 'las', 'Ingenierías', 'era', 'no', 'comprar', 'una', 'caja', 'negra', 'a', 'libro', 'cerrado', 'sino', 'abrir', 'su', 'contenido', 'para', 'no', 'encontrarse', 'con', 'sorpresas', 'desagradables.Y', 'Jorge', ',', 'tecnológo', ',', 'futbolero', 'y', 'tanguero', 'genial', 'no', 'encontró', 'mejor', 'analogía', 'para', 'el', 'caso', 'que', 'hablar', 'de', '“', 'abrir', 'el', 'paquete', '”', 'en', 'homenaje', 'a', 'Enrique', 'Santos', 'Discépolo', ':', '“', 'Me', 'da', 'tristeza', 'el', 'panete', ',', 'chicato', 'inocente', 'que', 'se', 'la', 'llevó…', '¡Cuando', 'desate', 'el', 'paquete', 'y', 'manye', 'que', 'se', 'ensartó', '!', '”', 'Los', 'conceptos', 'y', 'relaciones', 'se', 'suceden', ',', 'hasta', 'que', 'resulta', 'necesario', 'un', 'cambio', 'de', 'escala', 'para', 'tratar', 'las', 'relaciones', 'entre', 'Estado', ',', 'Empresas', 'e', 'Instituciones', 'científicas', ':', 'Si', '[', 'lo', 'que', 'estamos', 'conversando', ']', 'hubiese', 'sido', 'sobre', 'una', 'idea', 'que', 'los', 'investigadores', 'ya', 'tenían', ',', 'podría', 'haber', 'cabido', 'juntar', 'algunas', 'empresas', 'para', 'financiar', 'la', 'primera', 'etapa', 'del', 'estudio', '[', 'Investigación', 'precompetitiva', ']', 'Ello', ',', 'en', 'la', 'medida', 'que', 'éste', 'fuera', 'lo', 'suficientemente', 'general', 'como', 'para', 'que', 'pudiera', 'ser', 'explotado', 'luego', 'por', 'distintas', 'empresas', '.', '[', 'Inversión', 'de', 'riesgo', ']', 'Y', 'si', 'retrocediéramos', 'aún', 'un', 'paso', 'más', ',', 'Uds', '.', 'podrían', 'haber', 'sometido', 'un', 'listado', 'de', 'ideas', 'para', 'analizar', '[', 'Selección', 'de', 'ideas', 'de', 'negocio', ']', 'En', 'ello', ',', 'se', 'toca', 'el', 'punto', ':', '¿de', 'dónde', 'salen', 'las', 'ideas', '?', '¿De', 'un', 'grupo', 'de', 'científicos', ',', 'o', 'un', 'grupo', 'de', 'tecnólogos', '?', 'Tienen', 'que', 'salir', 'de', 'los', 'grupos', 'de', 'ventas', '.', 'Es', 'la', 'única', 'manera', 'de', 'responder', 'a', 'la', 'demanda', '.', 'En', 'una', 'empresa', ',', 'las', 'ideas', 'de', 'desarrollo', 'no', 'salen', 'de', 'la', 'Gerencia', 'de', 'Desarrollo', '.', 'Salen', 'de', 'la', 'Gerencia', 'de', 'Ventas', '.', 'De', 'esta', 'manera', ',', 'aparece', 'el', 'Cliente', 'como', 'entidad', 'en', 'la', 'visualización', '.', 'Y', 'cobra', 'sentido', 'la', 'infografía', ':', 'hacia', 'la', 'izquierda', ',', 'están', 'el', 'cliente', 'y', 'el', 'producto', 'tangible', '.', 'Y', 'hacia', 'la', 'derecha', ',', 'el', 'proceso', ',', 'la', 'investigación', ',', 'y', 'en', 'definitiva', 'y', 'de', 'forma', 'progresiva', ',', 'todo', 'aquello', 'que', 'el', 'Cliente', 'no', 've', ':', 'lo', 'que', 'hoy', 'en', 'un', 'mapa', 'de', 'Wardley', 'se', 'presenta', 'de', 'arriba', 'a', 'abajo', '.', 'En', 'esta', 'visualización', ',', 'el', 'nivel', 'de', 'abstracción', 'a', 'la', 'derecha', 'es', 'máximo', ':', 'ya', 'no', 'es', 'la', 'tecnología', ',', 'ni', 'la', 'Ciencia', ':', 'son', 'las', 'reflexiones', 'filosóficas', 'de', 'Florencio', 'Escardó', 'y', 'Ernesto', 'Sábato', '.', 'Surgen', 'la', 'cuestión', 'de', 'la', 'falta', 'de', 'interlocures', 'válidos', 'entre', 'Empresas', 'e', 'Instituciones', 'Científicas', ',', 'y', 'la', 'visión', 'del', 'Desarrollo', ':', 'a', 'corto', 'plazo', ',', 'se', 'busca', 'comprar', 'tecnologías', 'ya', 'probadas', ',', 'con', 'garantía', 'de', 'funcionamiento', 'y', 'asesoramiento', '.', 'Se', 'plantea', 'entonces', ':', '…el', 'dilema', 'del', 'subdesarrollo', ':', 'en', 'un', 'país', 'desarrollado', ',', 'a', 'una', 'empresa', 'que', 'no', 'invierte', 'en', 'innovar', 'le', 'puede', 'costar', 'muy', 'caro', ';', 'en', 'cambio', ',', 'en', 'un', 'país', 'subdesarrollado', ',', 'es', 'a', 'la', 'empresa', 'que', 'invierta', 'en', 'innovar', 'que', 'le', 'puede', 'costar', 'muy', 'caro', '.', 'Pero', 'al', 'mismo', 'tiempo', ',', 'si', 'no', 'innovamos', ',', 'no', 'saldremos…', 'En', 'la', 'charla', ',', 'surgen', 'las', 'cuestiones', 'legales', '(', 'ej', '.', 'patentes', ')', ',', 'su', 'marcoy', 'el', 'rol', 'del', 'estdo', 'entre', 'las', 'relaciones', 'Ciencia-Empresa', ':', 'En', 'el', 'caso', 'de', 'los', 'commodities', ',', 'Ud', '.', 'tiene', 'un', 'mercado', 'abierto', 'de', 'tecnología', '.', 'Pone', 'los', '“', 'verdes', '”', 'sobre', 'la', 'mesa', 'y', 'listo', '.', 'Se', 'trata', 'de', 'producciones', 'de', 'grandes', 'volúmenes', 'con', 'muchos', 'fierros', '.', '[', '…', ']', 'Sería', 'absurdo', 'poner', 'en', 'la', 'lista', 'de', 'prioridades', 'nacionales', 'al', 'desarrollo', 'de', 'commodities', '.', 'Sería', 'carísimo.A', 'su', 'vez', ',', 'hay', 'otros', 'casos', 'en', 'los', 'que', 'Ud', '.', 'se', 'encuentra', 'con', 'patentes', '.', 'O', 'bien', 'va', 'al', 'pie', 'y', 'paga', 'las', 'regalías', 'del', 'caso', ',', 'o', 'intenta', 'descifrar', 'la', 'patente', 'y', 'encontrar', 'una', 'variante', 'que', 'le', 'permita', 'esquivarla', '.', 'Hay', 'muchos', 'profesionales', 'expertos', 'en', 'esa', 'materia', '.', 'Con', 'lo', 'cual', ',', 'el', 'mismo', 'hecho', 'de', 'patentar', 'implica', 'un', 'riesgo.Por', 'eso', 'queda', 'la', 'tercera', 'variante', ':', 'en', 'la', 'que', 'el', 'secreto', 'es', 'tal', 'que', 'ni', 'siquiera', 'se', 'lo', 'patenta', '.', 'Roberto', 'Cunningham', 'plantea', 'la', 'épica', 'del', 'Maracanazo', 'para', 'desbancar', 'la', 'idea', 'del', 'foco', 'en', '“', 'Planes', 'de', 'Estudios', '”', 'que', 'ignoran', 'a', 'los', 'docentes', '.', '—', 'Ud', '.', 'ni', 'siquiera', 'había', 'nacido', ',', 'pero', 'seguramente', 'oyó', 'hablar', 'del', 'Uruguay', 'campeón', 'del', 'mundo', 'en', 'el', 'Maracaná', 'en', 'el', '50', '.', '–', 'Sí', ',', 'mi', 'padre', 'tenía', 'un', 'primo', 'uruguayo', 'y', 'cada', 'vez', 'que', 'se', 'juntaban', 'hablaban', 'del', 'Maracanazo', '.', '[', '…', ']', 'Nadie', 'daba', 'un', 'peso', 'por', 'el', 'equipo', 'uruguayo', '.', 'Se', 'habían', 'conocido', 'en', 'el', 'barco', 'de', 'ida', 'a', 'Río', '.', '–', 'Con', 'el', 'empate', ',', 'Brasil', 'era', 'campeón…', 'y', 'ganaba', 'por', '1', 'a', '0…', '¿Y', 'su', 'padre', 'y', 'su', 'primo', 'oriental', ',', 'le', 'hablaban', 'de', 'los', 'jugadores', '?', '–', 'Pero', 'cómo', 'no', ',', 'de', 'Obdulio', ',', 'de', 'Ghiggia', ',', 'de', 'Máspoli', '.', '¡Máspoli', '!', '—', 'Y', 'de', 'las', 'tácticas', '.', '¿No', 'hablaban', 'de', 'las', 'tácticas', '?', '.', '—', 'No', '.', '¿Qué', 'quiere', 'que', 'dijeran', 'de', 'las', 'tácticas', '?', '.', '—', 'No', 'quiero', 'nada', '.', 'Pero', 'fíjese', 'que', 'ese', 'equipo', ',', 'como', 'cualquier', 'otro', ',', 'pasa', 'a', 'la', 'historia', 'por', 'sus', 'jugadores', 'y', 'no', 'por', 'sus', 'tácticas.Las', 'tácticas', 'son', 'los', 'planes', 'de', 'estudio', '.', 'Los', 'jugadores', 'son', 'los', 'maestros.Déme', 'el', 'nombre', 'de', 'una', 'universidad', 'que', 'haya', 'pasado', 'a', 'la', 'historia', 'por', 'sus', 'planes', 'de', 'estudio', '.', 'En', 'cambio', ',', 'Aristóteles', ',', 'Francis', 'Bacon', ',', 'Newton', ',', 'Maxwell', '…', 'Al', 'final', 'de', 'la', 'obra', ',', 'la', 'infografía', 'queda', 'completa', ',', 'exponiendo', 'y', 'relacionando', 'todos', 'los', 'conceptos', 'desarrollados', '.', 'Queda', 'brevemente', 'en', 'pantalla', ',', 'y', 'se', 'apaga', 'lentamente', 'mientras', 'suena', 'la', 'música', 'de', 'cierre', '.', 'En', 'su', 'genial', 'obra', ',', 'Roberto', 'logró', 'entrelazar', 'ciencia', ',', 'tecnología', ',', 'políticas', 'públicas…', 'y', 'también', 'tango', 'y', 'fútbol', '.', 'En', 'sus', 'propias', 'palabras', ':', 'Jorge', 'Alberto', 'Sabato', 'fue', 'un', 'físico', 'que', 'dirigió', 'por', 'muchos', 'años', 'el', 'Departamento', 'de', 'Tecnología', 'de', 'Materiales', 'de', 'la', 'Comisión', 'Nacional', 'de', 'Energía', 'Atómica', 'y', 'es', 'el', 'adalid', 'y', 'mentor', 'de', 'la', 'gestión', 'tecnológica', 'en', 'América', 'latina.Florencio', 'Escardó', 'fue', 'médico', 'pediatra', 'de', 'dilatada', 'atención', 'en', 'el', 'campo', 'de', 'la', 'medicina', 'y', ',', 'antes', 'que', 'ello', ',', 'un', 'maestro', 'de', 'juventudes.Dante', 'Enrique', 'Panzeri', 'fue', 'un', 'periodista', 'deportivo', 'que', 'en', 'la', 'década', 'del', '’', '60', 'revolucionó', 'el', 'mundo', 'del', 'fútbol', 'desde', 'las', 'páginas', 'de', 'la', 'revista', 'El', 'Gráfico.Los', 'tres', ',', 'ya', 'desaparecidos', ',', 'constituyen', 'la', 'mejor', 'síntesis', 'y', 'representación', 'de', 'esa', 'trilogía', 'tecnología', ',', 'tango', ',', 'fútbol', '(', 'e', 'innovación', ')', 'a', 'que', 'aludimos', '.', 'Con', 'el', 'fallecimiento', 'del', 'Dr.', 'Cunningham', 'en', 'octubre', 'de', '2008', ',', 'la', 'comunidad', 'científica', 'y', 'empresarial', 'perdió', 'a', 'un', 'líder', 'visionario', 'que', 'supo', 'unir', 'mundos', '.', 'Trabajar', 'junto', 'a', 'él', 'fue', 'una', 'experiencia', 'inolvidable', ';', 'compartir', 'con', 'ustedes', 'el', 'producto', 'de', 'ese', 'encuentro', 'es', 'una', 'forma', 'de', 'recordarlo', ',', 'y', 'de', 'valorar', 'todos', 'los', 'aportes', 'aún', 'vigentes', 'de', 'su', 'mirada', 'al', 'desarrollo', 'tecnológico', 'y', 'la', 'innovación', '.', 'Que', 'su', 'memoria', 'siga', 'inspirándonos', 'a', 'seguir', 'explorando', 'nuevos', 'horizontes', 'y', 'aportando', 'lo', 'mejor', 'que', 'tenemos', 'para', 'construir', 'un', 'mundo', 'mejor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "uVw6jqAJjH0G"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis inicial**"
      ],
      "metadata": {
        "id": "apvEr6c7mly3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis Exploratorio**"
      ],
      "metadata": {
        "id": "8hbwwuZWnoq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Longitud de las Oraciones**"
      ],
      "metadata": {
        "id": "UpEIn0CDnwKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Frecuencia de Palabras**"
      ],
      "metadata": {
        "id": "Trlj8RDLpE-m"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'path' is a list of lists or a 2D numpy array containing strings:\n",
        "\n",
        "# Create a dictionary to store word frequencies\n",
        "frecuencia_total = {}\n",
        "\n",
        "# Iterate through all strings in 'path'\n",
        "for sublist in path:  # If path is a list of lists\n",
        "    for word in sublist:\n",
        "        frecuencia_total[word] = frecuencia_total.get(word, 0) + 1\n",
        "        # If word is not in dictionary, initialize to 0 and add 1\n",
        "        # If word is in dictionary, increment count by 1\n",
        "\n",
        "# Convert the dictionary to a list of (word, frequency) tuples\n",
        "frecuencia_total_list = list(frecuencia_total.items())\n",
        "\n",
        "# Sort the list by frequency in descending order\n",
        "frecuencia_total_list.sort(key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Get the top 5 most frequent words and their frequencies\n",
        "palabras_mas_frecuentes = [word for word, freq in frecuencia_total_list[:5]]\n",
        "frecuencias_mas_frecuentes = [freq for word, freq in frecuencia_total_list[:5]]\n",
        "\n",
        "# Print the results\n",
        "print(\"Palabras más frecuentes:\")\n",
        "for palabra, frecuencia in zip(palabras_mas_frecuentes, frecuencias_mas_frecuentes):\n",
        "    print(f\"{palabra}: {frecuencia}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEs2QraPwxag",
        "outputId": "8b3e57b5-53fc-42aa-fc1b-b89212cb931e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras más frecuentes:\n",
            "/: 8\n",
            "e: 5\n",
            "s: 5\n",
            "o: 4\n",
            "a: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de las Partes del Discurso (POS)**"
      ],
      "metadata": {
        "id": "aU25om3kowBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Descargar los taggers si es necesario\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Función para etiquetar las partes del discurso\n",
        "def pos_tagging(texto):\n",
        "    tokens = word_tokenize(texto)\n",
        "    return pos_tag(tokens)\n",
        "# Definir el DF\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'])\n",
        "print(df)\n",
        "# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\n",
        "df['pos_tags'] = df['texto'].apply(pos_tagging)\n",
        "\n",
        "# Mostrar las primeras 5 filas con las etiquetas POS\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "xynxJtKW3l_v",
        "outputId": "34107b40-1b31-417b-f2b7-3ea0a80c704b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-662bb5641502>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Mostrar las primeras 5 filas con las etiquetas POS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Descargar los taggers si es necesario\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Función para etiquetar las partes del discurso\n",
        "def pos_tagging(texto):\n",
        "    tokens = word_tokenize(texto)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Assuming your data is in a CSV file named 'your_data.csv'\n",
        "# Replace 'your_data.csv' with the actual file name\n",
        "df = pd.read_csv('your_data.csv')  # Load your data into a pandas DataFrame\n",
        "\n",
        "# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\n",
        "df['pos_tags'] = df['texto'].apply(pos_tagging)\n",
        "\n",
        "# Mostrar las primeras 5 filas con las etiquetas POS\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "L380ZKSc3uvo",
        "outputId": "4d714835-f2c7-4ea1-f8a4-06dc3a1a2038"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-94e273c5a21b>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assuming your data is in a CSV file named 'your_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Replace 'your_data.csv' with the actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load your data into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Distribución de Longitud de Palabras**"
      ],
      "metadata": {
        "id": "ImJPC5Ymo5X4"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np  # Import numpy for calculations\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for visualization\n",
        "\n",
        "\n",
        "# Replace 'your_text_data.csv' with the actual file path if it's not in the same directory\n",
        "file_path = 'your_text_data.csv'\n",
        "# or\n",
        "#file_path = '/path/to/your/your_text_data.csv' # if the file is not in the same dir\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'. Please check the file path.\")\n",
        "    # Handle the error gracefully, e.g., exit the script or use a default dataset\n",
        "    #exit()\n",
        "\n",
        "# ... (rest of the code remains the same) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ajtTR2-Amt",
        "outputId": "e8babd0a-c355-4f8e-f12a-16f8b8a30aa4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at 'your_text_data.csv'. Please check the file path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribución de la Frecuencia de Palabras Únicas**"
      ],
      "metadata": {
        "id": "N-AUpsbTqvVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Load your text data into a Pandas DataFrame\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Function to count word frequencies\n",
        "def word_frequency_distribution(text):\n",
        "    words = word_tokenize(text)\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Apply the function to the text column\n",
        "data['word_counts'] = data['text_column'].apply(word_frequency_distribution)\n",
        "\n",
        "# Flatten the list of word counts\n",
        "all_word_counts = Counter()\n",
        "for counts in data['word_counts']:\n",
        "    all_word_counts.update(counts)\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = all_word_counts.most_common(10)\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.bar([word[0] for word in most_common_words], [word[1] for word in most_common_words])\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Common Words\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7TtMnSjF-Y6z",
        "outputId": "dc8bd28d-e303-4d65-f1ad-403386e8b162"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-996db1b02e46>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load your text data into a Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Function to count word frequencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt  # Make sure to import pyplot\n",
        "\n",
        "# Replace 'your_text_data.csv' with the actual file path\n",
        "file_path = 'your_text_data.csv'\n",
        "# or if the file is not in the same dir as your code\n",
        "# file_path = '/path/to/your/your_text_data.csv'\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'. Please check the file path.\")\n",
        "    # Handle the error gracefully, e.g., exit the script or use a default dataset\n",
        "    # For now, let's raise the error to stop execution\n",
        "    raise\n",
        "\n",
        "# Function to count word frequencies\n",
        "def word_frequency_distribution(text):\n",
        "    words = word_tokenize(text)\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Assuming your text data column is named 'text_column'\n",
        "# Change 'text_column' to the actual name if different\n",
        "data['word_counts'] = data['text_column'].apply(word_frequency_distribution)\n",
        "\n",
        "# Flatten the list of word counts\n",
        "all_word_counts = Counter()\n",
        "for counts in data['word_counts']:\n",
        "    all_word_counts.update(counts)\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = all_word_counts.most_common(10)\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.bar([word[0] for word in most_common_words], [word[1] for word in most_common_words])\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Common Words\")\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CFKAdj-E-nFd",
        "outputId": "8307b5bf-4667-45ae-8a4a-e475352e39eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at 'your_text_data.csv'. Please check the file path.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_text_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e0586c233944>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: File not found at '{file_path}'. Please check the file path.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_text_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de N-gramas**"
      ],
      "metadata": {
        "id": "kfSRoRLrq7ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Diversidad Léxica**"
      ],
      "metadata": {
        "id": "h9bzRbdnrB-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de Palabras con Word Cloud**"
      ],
      "metadata": {
        "id": "0rx4_YF-rGhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones del Analisis exploratorio**"
      ],
      "metadata": {
        "id": "SDy994LxrLSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento del Texto**"
      ],
      "metadata": {
        "id": "DwXACyM-rUZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenización:\n",
        "\n",
        "Tokenizar el texto en palabras, frases o párrafos según sea necesario.\n",
        "\n",
        "Limpieza del Texto:\n",
        "\n",
        "Convertir a minusculas, eliminar caracteres no deseados, stopwords, lematización, y stemming.\n",
        "\n",
        "Análisis Léxico y Morfológico:\n",
        "\n",
        "Identificación de partes del discurso (POS tagging) y análisis morfológico."
      ],
      "metadata": {
        "id": "uzxU7IE7rYte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Sintáctico**"
      ],
      "metadata": {
        "id": "TAhIjPnlrlY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing:\n",
        "\n",
        "Construir árboles sintácticos para las oraciones en el corpus.\n",
        "\n",
        "Dependencia Sintáctica:\n",
        "\n",
        "Análisis de dependencias para entender las relaciones gramaticales entre palabras."
      ],
      "metadata": {
        "id": "ofAV8fVdrxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Semántico**"
      ],
      "metadata": {
        "id": "C_zqx-0pr3_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coreferencia:\n",
        "\n",
        "Resolver las referencias cruzadas en el texto para entender a qué se refieren los pronombres y otras expresiones.\n",
        "\n",
        "Análisis de Coherencia:\n",
        "\n",
        "Evaluar la coherencia y cohesión del discurso."
      ],
      "metadata": {
        "id": "gZSiRXc0sOm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Análisis Pragmático**"
      ],
      "metadata": {
        "id": "hH99SSBzsXc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de Sentimientos:\n",
        "\n",
        "Determinar el tono y las emociones expresadas en el texto.\n",
        "\n",
        "Detección de Intenciones:\n",
        "\n",
        "Identificar las intenciones detrás de las expresiones del texto."
      ],
      "metadata": {
        "id": "hUWts_4usgo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Codificación de texto a vectores**"
      ],
      "metadata": {
        "id": "SFh5txD0slcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of word\n",
        "\n",
        "Tf-IDF\n",
        "\n",
        "Word Embendings"
      ],
      "metadata": {
        "id": "imQS1D29sqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "J1QMfHxvsusd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de variable objetivo y variables independientes**"
      ],
      "metadata": {
        "id": "ffwi-JVpsz4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificación de Texto**\n",
        "\n",
        "X (Entrada): Texto del documento (puede ser una oración, párrafo, o artículo completo).\n",
        "\n",
        "Y (Salida): Etiqueta de la categoría del texto (por ejemplo, \"positivo\" o \"negativo\" para análisis de sentimientos, o categorías como \"deportes\", \"política\", \"tecnología\" para clasificación de noticias).\n",
        "\n",
        "**Análisis de Sentimientos**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Sentimiento asociado (por ejemplo, \"positivo\", \"negativo\" o \"neutral\").\n",
        "\n",
        "**Reconocimiento de Entidades Nombradas (NER)**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Entidades reconocidas y sus categorías (por ejemplo, \"PERSONA\", \"ORGANIZACIÓN\", \"LUGAR\").\n",
        "\n",
        "**Traducción Automática**\n",
        "\n",
        "X (Entrada): Texto en el idioma de origen.\n",
        "\n",
        "Y (Salida): Texto traducido al idioma de destino.\n",
        "\n",
        "**Resumen de Texto**\n",
        "\n",
        "X (Entrada): Texto completo del documento.\n",
        "\n",
        "Y (Salida): Resumen del documento.\n",
        "\n",
        "Generación de Texto **texto en negrita**\n",
        "\n",
        "X (Entrada): Prompt o inicio de una frase o párrafo.\n",
        "\n",
        "Y (Salida): Texto generado continuando el prompt.\n",
        "\n",
        "**Modelos de Lenguaje (Language Modeling)**\n",
        "\n",
        "X (Entrada): Una secuencia de palabras o caracteres.\n",
        "\n",
        "Y (Salida): La siguiente palabra o carácter en la secuencia."
      ],
      "metadata": {
        "id": "TIAOc0vus5HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos**"
      ],
      "metadata": {
        "id": "udkFVq-MttGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Librerias necesarias para implementar los modelos**"
      ],
      "metadata": {
        "id": "1RjM87TLtvuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**División de datos en conjuntos de entrenamiento y prueba**"
      ],
      "metadata": {
        "id": "xqm8o1akt4Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicción con conjunto de prueba**"
      ],
      "metadata": {
        "id": "kwL8NaD5t9VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del rendimiento del modelo**"
      ],
      "metadata": {
        "id": "DQYju67PuCrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre el modelado y las metricas**"
      ],
      "metadata": {
        "id": "UmqwS1z9uLo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimización de modelos**"
      ],
      "metadata": {
        "id": "7v8OyPWWuRDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre la optimización**"
      ],
      "metadata": {
        "id": "IcA9t9msudUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "stRic8RiufKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos utilizando Redes Neuronales**"
      ],
      "metadata": {
        "id": "tszqOKGEuvDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definición del problema**"
      ],
      "metadata": {
        "id": "y-mf6nIku09T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determinar qué quieres lograr con la red neuronal (clasificación, regresión, predicción de series temporales, etc.).\n",
        "\n",
        "Identificar las variables de entrada (features) y la salida esperada (labels o targets).\n",
        "\n",
        "Según el problema, seleccionar el tipo adecuado: Perceptrón Multicapa (MLP), CNN, RNN, etc."
      ],
      "metadata": {
        "id": "eD_WpQ39u8x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diseño de la red neuronal**"
      ],
      "metadata": {
        "id": "lmAiNXTIvBG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estructura:\n",
        "\n",
        "Seleccionar la cantidad de capas (profundidad) y neuronas por capa (ancho). Escoger la función de activación para cada capa (ReLU, Sigmoid, Softmax, etc.).\n",
        "\n",
        "Conexiones:\n",
        "\n",
        "Definir cómo se conectan las capas (densa, convolucional, recurrente, etc.). Pérdida y optimización:\n",
        "\n",
        "Elegir una función de pérdida (Cross-Entropy, MSE, etc.) según el problema. Seleccionar un optimizador (SGD, Adam, etc.)."
      ],
      "metadata": {
        "id": "tUB8XT4VvRoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "kPJVQ_KwvV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialización: Iniciar pesos y sesgos de manera adecuada (aleatorio, Xavier, He, etc.).\n",
        "\n",
        "Propagación hacia adelante (Forward pass): Calcular las predicciones. Cálculo de pérdida: Comparar las predicciones con las etiquetas esperadas.\n",
        "\n",
        "Propagación hacia atrás (Backpropagation): Ajustar los pesos utilizando el gradiente descendente.\n",
        "\n",
        "Iteraciones (Epochs): Repetir los pasos anteriores hasta alcanzar un buen desempeño o una condición de parada."
      ],
      "metadata": {
        "id": "LyH3PwU1va2a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26t6OQABvkwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validación y ajuste**"
      ],
      "metadata": {
        "id": "GYJFPk9Yve54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del modelo**"
      ],
      "metadata": {
        "id": "PTpPrzwNvl5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de resultados**"
      ],
      "metadata": {
        "id": "qmzFsz2kvp5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "NgivsfisvuAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tipos de modelo de Redes Neuronales**"
      ],
      "metadata": {
        "id": "PR8pb3vZv-Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo CNN es bueno para datos espaciales (como imágenes).\n",
        "\n",
        "El modelo RNN es ideal para datos secuenciales o dependientes del tiempo."
      ],
      "metadata": {
        "id": "g0n04s_3wEG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo CNN Típico**"
      ],
      "metadata": {
        "id": "riexdohYwJfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa convolucional:**\n",
        "\n",
        "Detecta patrones básicos en las imágenes, como bordes o texturas.\n",
        "\n",
        "**MaxPooling:**\n",
        "\n",
        "Reduce la dimensionalidad espacial para simplificar el modelo.\n",
        "\n",
        "**Flatten:**\n",
        "\n",
        "Convierte la salida bidimensional en un vector para pasar a las capas densas.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Previene el sobreajuste al apagar algunas neuronas aleatoriamente.\n",
        "\n",
        "Capa densa (Activación Sigmoide o Softmax):\n",
        "\n",
        "Genera probabilidades de pertenencia para cada clase."
      ],
      "metadata": {
        "id": "uPdApTHcwN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo RNN Típico**"
      ],
      "metadata": {
        "id": "pLd2MxCQwXc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa Embedding:**\n",
        "\n",
        "Convierte palabras o categorías en vectores densos\n",
        "\n",
        "**Capa RNN, LSTM o GRU:**\n",
        "\n",
        "Captura dependencias temporales en las secuencias de datos.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Reduce el riesgo de sobreajuste.\n",
        "\n",
        "**Capas densas:**\n",
        "Procesan las características aprendidas para clasificar las secuencias."
      ],
      "metadata": {
        "id": "H7rkTKvSwcxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wcN6dwzwm76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfSmQYQ5vkJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z-elUerjs_au"
      }
    }
  ]
}