{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrXH5PfESGizBPMatmGB0M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carocschall/CoderHouse/blob/main/DataScienceIII_CortezSchall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA SCIENCE III: NLP Y DEEP LEARNING APLICADO A LA CIENCIA DE DATOS - Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "XPiusanhkBwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alumna: Carolina Cortez Schall"
      ],
      "metadata": {
        "id": "5DGXRpoblUgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Presentación del proyecto**"
      ],
      "metadata": {
        "id": "BC3puDGClbTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstracto con Motivación y Audiencia\n",
        "\n",
        "Contexto Comercial y Analítico\n",
        "\n",
        "Preguntas/Hipótesis a Resolver\n",
        "\n",
        "Objetivo"
      ],
      "metadata": {
        "id": "dItLUymjliTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lectura de datos**"
      ],
      "metadata": {
        "id": "IbjVHbDdmI-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Librerias necesarias**"
      ],
      "metadata": {
        "id": "ZuQ77pqVmLAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEvGv6o3EOTX",
        "outputId": "97960676-a53c-41f7-b114-0c1c42c26a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: symspellpy in /usr/local/lib/python3.10/dist-packages (6.7.8)\n",
            "Requirement already satisfied: editdistpy>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from symspellpy) (0.1.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "! pip install -U symspellpy\n",
        "import nltk # importar natural language toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords') # modulo para descargar stopwords en diferentes idiomas\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import re\n",
        "import string\n",
        "import plotly\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "import time\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud\n",
        "import pickle\n",
        "from symspellpy import SymSpell\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Carga de datos**"
      ],
      "metadata": {
        "id": "TtZ01TRqQ8oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# El ensayo a analizar se encuentra en Innovaciones Educativas UNED\n",
        "url = \"https://revistas.uned.ac.cr/index.php/innovaciones/article/view/3299/4646\"\n",
        "\n",
        "# El código suele traer contenido que no es relevante para análisis. Para esto identificamos frases para identificar el inicio y el fin del contenido relevante\n",
        "start_index = \"SÍNTESIS Y REFLEXIONES FINALES\"\n",
        "end_index = \"REFERENCIAS\"\n",
        "\n",
        "# Enviar una solicitud GET para obtener el contenido HTML bruto\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200: #valida si la página carga correctamente\n",
        "    # Parsear el contenido HTML usando BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "\n",
        "    # Extraer la porción relevante del texto\n",
        "    start_index = article_text.find(frase_inicial)\n",
        "    end_index = article_text.find(frase_final) + len(frase_final)\n",
        "#Si ambas frases (inicial y final) se encuentran en el texto, se extrae el bloque de texto entre esas frases.\n",
        "    if start_index != -1 and end_index != -1:\n",
        "        texto_relevante = article_text[start_index:end_index]\n",
        "        print(texto_relevante)\n",
        "    else:\n",
        "        print(\"No se encontraron las frases especificadas en el artículo.\")\n",
        "else:\n",
        "    print(f\"Fallo al recuperar la página web. Código de estado: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMXcSWWZ77Bw",
        "outputId": "8ba2c3a1-1ee8-4979-c5ff-07086ff79fd5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No se encontraron las frases especificadas en el artículo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lectura del corpus**"
      ],
      "metadata": {
        "id": "OHrgcMYsmirp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Aplicamos metodo  para tokenizar palabras\n",
        "\n",
        "tokenized_palabra=word_tokenize(texto_relevante)\n",
        "print(tokenized_palabra)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "pOv2S4TYUDvT",
        "outputId": "67e2dc75-aa6a-49ac-afeb-2ddda066fbfa"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'texto_relevante' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-e28a6cd6c443>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Aplicamos metodo  para tokenizar palabras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenized_palabra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_relevante\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_palabra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'texto_relevante' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install gdown\n",
        "\n",
        "import gdown\n",
        "\n",
        "# Replace 'YOUR_GOOGLE_DRIVE_LINK' with your actual link\n",
        "url = 'https://drive.google.com/file/d/18xvZRTWqKOszOcBf-HtuxVIW9EFu1fS8/view?usp=sharing'\n",
        "output = 'downloaded_dataset.zip' # Choose your desired output filename\n",
        "\n",
        "# Download the file\n",
        "gdown.download(url=url, output=output, quiet=False)\n",
        "\n",
        "# Unzip the file if necessary\n",
        "!unzip downloaded_dataset.zip"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "692utkmJI7tD",
        "outputId": "49b7b967-403c-4e60-a1c9-0b3460f21afd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=18xvZRTWqKOszOcBf-HtuxVIW9EFu1fS8\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/file/d/18xvZRTWqKOszOcBf-HtuxVIW9EFu1fS8/view?usp=sharing\n",
            "To: /content/downloaded_dataset.zip\n",
            "92.3kB [00:00, 2.36MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  downloaded_dataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of downloaded_dataset.zip or\n",
            "        downloaded_dataset.zip.zip, and cannot find downloaded_dataset.zip.ZIP, period.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "uVw6jqAJjH0G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis inicial**"
      ],
      "metadata": {
        "id": "apvEr6c7mly3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis Exploratorio**"
      ],
      "metadata": {
        "id": "8hbwwuZWnoq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Longitud de las Oraciones**"
      ],
      "metadata": {
        "id": "UpEIn0CDnwKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Frecuencia de Palabras**"
      ],
      "metadata": {
        "id": "Trlj8RDLpE-m"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'path' is a list of lists or a 2D numpy array containing strings:\n",
        "\n",
        "# Create a dictionary to store word frequencies\n",
        "frecuencia_total = {}\n",
        "\n",
        "# Iterate through all strings in 'path'\n",
        "for sublist in path:  # If path is a list of lists\n",
        "    for word in sublist:\n",
        "        frecuencia_total[word] = frecuencia_total.get(word, 0) + 1\n",
        "        # If word is not in dictionary, initialize to 0 and add 1\n",
        "        # If word is in dictionary, increment count by 1\n",
        "\n",
        "# Convert the dictionary to a list of (word, frequency) tuples\n",
        "frecuencia_total_list = list(frecuencia_total.items())\n",
        "\n",
        "# Sort the list by frequency in descending order\n",
        "frecuencia_total_list.sort(key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Get the top 5 most frequent words and their frequencies\n",
        "palabras_mas_frecuentes = [word for word, freq in frecuencia_total_list[:5]]\n",
        "frecuencias_mas_frecuentes = [freq for word, freq in frecuencia_total_list[:5]]\n",
        "\n",
        "# Print the results\n",
        "print(\"Palabras más frecuentes:\")\n",
        "for palabra, frecuencia in zip(palabras_mas_frecuentes, frecuencias_mas_frecuentes):\n",
        "    print(f\"{palabra}: {frecuencia}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEs2QraPwxag",
        "outputId": "8b3e57b5-53fc-42aa-fc1b-b89212cb931e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras más frecuentes:\n",
            "/: 8\n",
            "e: 5\n",
            "s: 5\n",
            "o: 4\n",
            "a: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de las Partes del Discurso (POS)**"
      ],
      "metadata": {
        "id": "aU25om3kowBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Descargar los taggers si es necesario\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Función para etiquetar las partes del discurso\n",
        "def pos_tagging(texto):\n",
        "    tokens = word_tokenize(texto)\n",
        "    return pos_tag(tokens)\n",
        "# Definir el DF\n",
        "data = [['Alex',10],['Bob',12],['Clarke',13]]\n",
        "df = pd.DataFrame(data,columns=['Name','Age'])\n",
        "print(df)\n",
        "# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\n",
        "df['pos_tags'] = df['texto'].apply(pos_tagging)\n",
        "\n",
        "# Mostrar las primeras 5 filas con las etiquetas POS\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "xynxJtKW3l_v",
        "outputId": "34107b40-1b31-417b-f2b7-3ea0a80c704b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-662bb5641502>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_tags'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'texto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagging\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Mostrar las primeras 5 filas con las etiquetas POS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Descargar los taggers si es necesario\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Función para etiquetar las partes del discurso\n",
        "def pos_tagging(texto):\n",
        "    tokens = word_tokenize(texto)\n",
        "    return pos_tag(tokens)\n",
        "\n",
        "# Assuming your data is in a CSV file named 'your_data.csv'\n",
        "# Replace 'your_data.csv' with the actual file name\n",
        "df = pd.read_csv('your_data.csv')  # Load your data into a pandas DataFrame\n",
        "\n",
        "# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\n",
        "df['pos_tags'] = df['texto'].apply(pos_tagging)\n",
        "\n",
        "# Mostrar las primeras 5 filas con las etiquetas POS\n",
        "print(df.head())"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "L380ZKSc3uvo",
        "outputId": "4d714835-f2c7-4ea1-f8a4-06dc3a1a2038"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-94e273c5a21b>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Assuming your data is in a CSV file named 'your_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Replace 'your_data.csv' with the actual file name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load your data into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Aplicar la función a la columna 'texto' y crear una nueva columna 'pos_tags'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Distribución de Longitud de Palabras**"
      ],
      "metadata": {
        "id": "ImJPC5Ymo5X4"
      }
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np  # Import numpy for calculations\n",
        "import matplotlib.pyplot as plt  # Import matplotlib for visualization\n",
        "\n",
        "\n",
        "# Replace 'your_text_data.csv' with the actual file path if it's not in the same directory\n",
        "file_path = 'your_text_data.csv'\n",
        "# or\n",
        "#file_path = '/path/to/your/your_text_data.csv' # if the file is not in the same dir\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'. Please check the file path.\")\n",
        "    # Handle the error gracefully, e.g., exit the script or use a default dataset\n",
        "    #exit()\n",
        "\n",
        "# ... (rest of the code remains the same) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ajtTR2-Amt",
        "outputId": "e8babd0a-c355-4f8e-f12a-16f8b8a30aa4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at 'your_text_data.csv'. Please check the file path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribución de la Frecuencia de Palabras Únicas**"
      ],
      "metadata": {
        "id": "N-AUpsbTqvVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Load your text data into a Pandas DataFrame\n",
        "data = pd.read_csv('data.csv')\n",
        "\n",
        "# Function to count word frequencies\n",
        "def word_frequency_distribution(text):\n",
        "    words = word_tokenize(text)\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Apply the function to the text column\n",
        "data['word_counts'] = data['text_column'].apply(word_frequency_distribution)\n",
        "\n",
        "# Flatten the list of word counts\n",
        "all_word_counts = Counter()\n",
        "for counts in data['word_counts']:\n",
        "    all_word_counts.update(counts)\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = all_word_counts.most_common(10)\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.bar([word[0] for word in most_common_words], [word[1] for word in most_common_words])\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Common Words\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7TtMnSjF-Y6z",
        "outputId": "dc8bd28d-e303-4d65-f1ad-403386e8b162"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-996db1b02e46>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load your text data into a Pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Function to count word frequencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.csv'"
          ]
        }
      ]
    },
    {
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt  # Make sure to import pyplot\n",
        "\n",
        "# Replace 'your_text_data.csv' with the actual file path\n",
        "file_path = 'your_text_data.csv'\n",
        "# or if the file is not in the same dir as your code\n",
        "# file_path = '/path/to/your/your_text_data.csv'\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'. Please check the file path.\")\n",
        "    # Handle the error gracefully, e.g., exit the script or use a default dataset\n",
        "    # For now, let's raise the error to stop execution\n",
        "    raise\n",
        "\n",
        "# Function to count word frequencies\n",
        "def word_frequency_distribution(text):\n",
        "    words = word_tokenize(text)\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Assuming your text data column is named 'text_column'\n",
        "# Change 'text_column' to the actual name if different\n",
        "data['word_counts'] = data['text_column'].apply(word_frequency_distribution)\n",
        "\n",
        "# Flatten the list of word counts\n",
        "all_word_counts = Counter()\n",
        "for counts in data['word_counts']:\n",
        "    all_word_counts.update(counts)\n",
        "\n",
        "# Get the most common words\n",
        "most_common_words = all_word_counts.most_common(10)\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.bar([word[0] for word in most_common_words], [word[1] for word in most_common_words])\n",
        "plt.xlabel(\"Word\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Most Common Words\")\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CFKAdj-E-nFd",
        "outputId": "8307b5bf-4667-45ae-8a4a-e475352e39eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at 'your_text_data.csv'. Please check the file path.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_text_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e0586c233944>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error: File not found at '{file_path}'. Please check the file path.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_text_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de N-gramas**"
      ],
      "metadata": {
        "id": "kfSRoRLrq7ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Diversidad Léxica**"
      ],
      "metadata": {
        "id": "h9bzRbdnrB-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de Palabras con Word Cloud**"
      ],
      "metadata": {
        "id": "0rx4_YF-rGhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones del Analisis exploratorio**"
      ],
      "metadata": {
        "id": "SDy994LxrLSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento del Texto**"
      ],
      "metadata": {
        "id": "DwXACyM-rUZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenización:\n",
        "\n",
        "Tokenizar el texto en palabras, frases o párrafos según sea necesario.\n",
        "\n",
        "Limpieza del Texto:\n",
        "\n",
        "Convertir a minusculas, eliminar caracteres no deseados, stopwords, lematización, y stemming.\n",
        "\n",
        "Análisis Léxico y Morfológico:\n",
        "\n",
        "Identificación de partes del discurso (POS tagging) y análisis morfológico."
      ],
      "metadata": {
        "id": "uzxU7IE7rYte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Sintáctico**"
      ],
      "metadata": {
        "id": "TAhIjPnlrlY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing:\n",
        "\n",
        "Construir árboles sintácticos para las oraciones en el corpus.\n",
        "\n",
        "Dependencia Sintáctica:\n",
        "\n",
        "Análisis de dependencias para entender las relaciones gramaticales entre palabras."
      ],
      "metadata": {
        "id": "ofAV8fVdrxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Semántico**"
      ],
      "metadata": {
        "id": "C_zqx-0pr3_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coreferencia:\n",
        "\n",
        "Resolver las referencias cruzadas en el texto para entender a qué se refieren los pronombres y otras expresiones.\n",
        "\n",
        "Análisis de Coherencia:\n",
        "\n",
        "Evaluar la coherencia y cohesión del discurso."
      ],
      "metadata": {
        "id": "gZSiRXc0sOm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Análisis Pragmático**"
      ],
      "metadata": {
        "id": "hH99SSBzsXc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de Sentimientos:\n",
        "\n",
        "Determinar el tono y las emociones expresadas en el texto.\n",
        "\n",
        "Detección de Intenciones:\n",
        "\n",
        "Identificar las intenciones detrás de las expresiones del texto."
      ],
      "metadata": {
        "id": "hUWts_4usgo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Codificación de texto a vectores**"
      ],
      "metadata": {
        "id": "SFh5txD0slcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of word\n",
        "\n",
        "Tf-IDF\n",
        "\n",
        "Word Embendings"
      ],
      "metadata": {
        "id": "imQS1D29sqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "J1QMfHxvsusd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de variable objetivo y variables independientes**"
      ],
      "metadata": {
        "id": "ffwi-JVpsz4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificación de Texto**\n",
        "\n",
        "X (Entrada): Texto del documento (puede ser una oración, párrafo, o artículo completo).\n",
        "\n",
        "Y (Salida): Etiqueta de la categoría del texto (por ejemplo, \"positivo\" o \"negativo\" para análisis de sentimientos, o categorías como \"deportes\", \"política\", \"tecnología\" para clasificación de noticias).\n",
        "\n",
        "**Análisis de Sentimientos**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Sentimiento asociado (por ejemplo, \"positivo\", \"negativo\" o \"neutral\").\n",
        "\n",
        "**Reconocimiento de Entidades Nombradas (NER)**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Entidades reconocidas y sus categorías (por ejemplo, \"PERSONA\", \"ORGANIZACIÓN\", \"LUGAR\").\n",
        "\n",
        "**Traducción Automática**\n",
        "\n",
        "X (Entrada): Texto en el idioma de origen.\n",
        "\n",
        "Y (Salida): Texto traducido al idioma de destino.\n",
        "\n",
        "**Resumen de Texto**\n",
        "\n",
        "X (Entrada): Texto completo del documento.\n",
        "\n",
        "Y (Salida): Resumen del documento.\n",
        "\n",
        "Generación de Texto **texto en negrita**\n",
        "\n",
        "X (Entrada): Prompt o inicio de una frase o párrafo.\n",
        "\n",
        "Y (Salida): Texto generado continuando el prompt.\n",
        "\n",
        "**Modelos de Lenguaje (Language Modeling)**\n",
        "\n",
        "X (Entrada): Una secuencia de palabras o caracteres.\n",
        "\n",
        "Y (Salida): La siguiente palabra o carácter en la secuencia."
      ],
      "metadata": {
        "id": "TIAOc0vus5HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos**"
      ],
      "metadata": {
        "id": "udkFVq-MttGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Librerias necesarias para implementar los modelos**"
      ],
      "metadata": {
        "id": "1RjM87TLtvuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**División de datos en conjuntos de entrenamiento y prueba**"
      ],
      "metadata": {
        "id": "xqm8o1akt4Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicción con conjunto de prueba**"
      ],
      "metadata": {
        "id": "kwL8NaD5t9VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del rendimiento del modelo**"
      ],
      "metadata": {
        "id": "DQYju67PuCrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre el modelado y las metricas**"
      ],
      "metadata": {
        "id": "UmqwS1z9uLo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimización de modelos**"
      ],
      "metadata": {
        "id": "7v8OyPWWuRDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre la optimización**"
      ],
      "metadata": {
        "id": "IcA9t9msudUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "stRic8RiufKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos utilizando Redes Neuronales**"
      ],
      "metadata": {
        "id": "tszqOKGEuvDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definición del problema**"
      ],
      "metadata": {
        "id": "y-mf6nIku09T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determinar qué quieres lograr con la red neuronal (clasificación, regresión, predicción de series temporales, etc.).\n",
        "\n",
        "Identificar las variables de entrada (features) y la salida esperada (labels o targets).\n",
        "\n",
        "Según el problema, seleccionar el tipo adecuado: Perceptrón Multicapa (MLP), CNN, RNN, etc."
      ],
      "metadata": {
        "id": "eD_WpQ39u8x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diseño de la red neuronal**"
      ],
      "metadata": {
        "id": "lmAiNXTIvBG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estructura:\n",
        "\n",
        "Seleccionar la cantidad de capas (profundidad) y neuronas por capa (ancho). Escoger la función de activación para cada capa (ReLU, Sigmoid, Softmax, etc.).\n",
        "\n",
        "Conexiones:\n",
        "\n",
        "Definir cómo se conectan las capas (densa, convolucional, recurrente, etc.). Pérdida y optimización:\n",
        "\n",
        "Elegir una función de pérdida (Cross-Entropy, MSE, etc.) según el problema. Seleccionar un optimizador (SGD, Adam, etc.)."
      ],
      "metadata": {
        "id": "tUB8XT4VvRoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "kPJVQ_KwvV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialización: Iniciar pesos y sesgos de manera adecuada (aleatorio, Xavier, He, etc.).\n",
        "\n",
        "Propagación hacia adelante (Forward pass): Calcular las predicciones. Cálculo de pérdida: Comparar las predicciones con las etiquetas esperadas.\n",
        "\n",
        "Propagación hacia atrás (Backpropagation): Ajustar los pesos utilizando el gradiente descendente.\n",
        "\n",
        "Iteraciones (Epochs): Repetir los pasos anteriores hasta alcanzar un buen desempeño o una condición de parada."
      ],
      "metadata": {
        "id": "LyH3PwU1va2a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26t6OQABvkwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validación y ajuste**"
      ],
      "metadata": {
        "id": "GYJFPk9Yve54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del modelo**"
      ],
      "metadata": {
        "id": "PTpPrzwNvl5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de resultados**"
      ],
      "metadata": {
        "id": "qmzFsz2kvp5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "NgivsfisvuAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tipos de modelo de Redes Neuronales**"
      ],
      "metadata": {
        "id": "PR8pb3vZv-Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo CNN es bueno para datos espaciales (como imágenes).\n",
        "\n",
        "El modelo RNN es ideal para datos secuenciales o dependientes del tiempo."
      ],
      "metadata": {
        "id": "g0n04s_3wEG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo CNN Típico**"
      ],
      "metadata": {
        "id": "riexdohYwJfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa convolucional:**\n",
        "\n",
        "Detecta patrones básicos en las imágenes, como bordes o texturas.\n",
        "\n",
        "**MaxPooling:**\n",
        "\n",
        "Reduce la dimensionalidad espacial para simplificar el modelo.\n",
        "\n",
        "**Flatten:**\n",
        "\n",
        "Convierte la salida bidimensional en un vector para pasar a las capas densas.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Previene el sobreajuste al apagar algunas neuronas aleatoriamente.\n",
        "\n",
        "Capa densa (Activación Sigmoide o Softmax):\n",
        "\n",
        "Genera probabilidades de pertenencia para cada clase."
      ],
      "metadata": {
        "id": "uPdApTHcwN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo RNN Típico**"
      ],
      "metadata": {
        "id": "pLd2MxCQwXc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa Embedding:**\n",
        "\n",
        "Convierte palabras o categorías en vectores densos\n",
        "\n",
        "**Capa RNN, LSTM o GRU:**\n",
        "\n",
        "Captura dependencias temporales en las secuencias de datos.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Reduce el riesgo de sobreajuste.\n",
        "\n",
        "**Capas densas:**\n",
        "Procesan las características aprendidas para clasificar las secuencias."
      ],
      "metadata": {
        "id": "H7rkTKvSwcxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wcN6dwzwm76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfSmQYQ5vkJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z-elUerjs_au"
      }
    }
  ]
}