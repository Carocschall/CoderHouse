{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwVgF9tBt/8Np1i7tOFOrS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carocschall/CoderHouse/blob/main/DataScienceIII_CortezSchall.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA SCIENCE III: NLP Y DEEP LEARNING APLICADO A LA CIENCIA DE DATOS - Sentiment Analysis**\n",
        "\n"
      ],
      "metadata": {
        "id": "XPiusanhkBwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alumna: Carolina Cortez Schall"
      ],
      "metadata": {
        "id": "5DGXRpoblUgH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Presentación del proyecto**"
      ],
      "metadata": {
        "id": "BC3puDGClbTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstracto con Motivación y Audiencia\n",
        "\n",
        "Contexto Comercial y Analítico\n",
        "\n",
        "Preguntas/Hipótesis a Resolver\n",
        "\n",
        "Objetivo"
      ],
      "metadata": {
        "id": "dItLUymjliTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lectura de datos**"
      ],
      "metadata": {
        "id": "IbjVHbDdmI-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Librerias necesarias**"
      ],
      "metadata": {
        "id": "ZuQ77pqVmLAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEvGv6o3EOTX",
        "outputId": "9be1ec05-c33d-4ba3-985e-5428f6bae9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting symspellpy\n",
            "  Downloading symspellpy-6.7.8-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting editdistpy>=0.1.3 (from symspellpy)\n",
            "  Downloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
            "Downloading symspellpy-6.7.8-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading editdistpy-0.1.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\n",
            "Successfully installed editdistpy-0.1.5 symspellpy-6.7.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "<ipython-input-2-37b7ca46bafc>:27: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download es_core_news_sm\n",
        "! pip install -U symspellpy\n",
        "import nltk # importar natural language toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords') # modulo para descargar stopwords en diferentes idiomas\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "import numpy  as np\n",
        "import re\n",
        "import string\n",
        "import plotly\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "import time\n",
        "import spacy\n",
        "import es_core_news_sm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.probability import FreqDist\n",
        "from wordcloud import WordCloud\n",
        "import pickle\n",
        "from symspellpy import SymSpell\n",
        "import pkg_resources\n",
        "from symspellpy import SymSpell, Verbosity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lectura del corpus**"
      ],
      "metadata": {
        "id": "OHrgcMYsmirp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the latest version.\n",
        "kagglehub.dataset_download('bricevergnou/spotify-recommendation')\n",
        "\n",
        "# Download a specific version.\n",
        "kagglehub.dataset_download('bricevergnou/spotify-recommendation/versions/1')\n",
        "\n",
        "# Download a single file\n",
        "kagglehub.dataset_download('bricevergnou/spotify-recommendation', path='data.csv')\n",
        "\n",
        "# Download a dataset or file, even if previously downloaded to cache.\n",
        "kagglehub.dataset_download('bricevergnou/spotify-recommendation', force_download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "oIkndlpLwhsd",
        "outputId": "b91068f5-3ede-4c42-f785-2b7f075ffc90"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bricevergnou/spotify-recommendation?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 16.3MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bricevergnou/spotify-recommendation?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.94k/5.94k [00:00<00:00, 5.76MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download already complete (14374 bytes).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bricevergnou/spotify-recommendation?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 20.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/kagglehub/datasets/bricevergnou/spotify-recommendation/versions/2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "metadata": {
        "id": "fkK4U0g9nvGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96da397a-9949-4bb4-e853-1cd94cd52c85"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-md==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.7.0/es_core_news_md-3.7.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-md==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-md==3.7.0) (0.1.2)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()"
      ],
      "metadata": {
        "id": "uVw6jqAJjH0G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis inicial**"
      ],
      "metadata": {
        "id": "apvEr6c7mly3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Análisis Exploratorio**"
      ],
      "metadata": {
        "id": "8hbwwuZWnoq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Longitud de las Oraciones**"
      ],
      "metadata": {
        "id": "UpEIn0CDnwKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Frecuencia de Palabras**"
      ],
      "metadata": {
        "id": "Trlj8RDLpE-m"
      }
    },
    {
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'path' is a list of lists or a 2D numpy array containing strings:\n",
        "\n",
        "# Create a dictionary to store word frequencies\n",
        "frecuencia_total = {}\n",
        "\n",
        "# Iterate through all strings in 'path'\n",
        "for sublist in path:  # If path is a list of lists\n",
        "    for word in sublist:\n",
        "        frecuencia_total[word] = frecuencia_total.get(word, 0) + 1\n",
        "        # If word is not in dictionary, initialize to 0 and add 1\n",
        "        # If word is in dictionary, increment count by 1\n",
        "\n",
        "# Convert the dictionary to a list of (word, frequency) tuples\n",
        "frecuencia_total_list = list(frecuencia_total.items())\n",
        "\n",
        "# Sort the list by frequency in descending order\n",
        "frecuencia_total_list.sort(key=lambda item: item[1], reverse=True)\n",
        "\n",
        "# Get the top 5 most frequent words and their frequencies\n",
        "palabras_mas_frecuentes = [word for word, freq in frecuencia_total_list[:5]]\n",
        "frecuencias_mas_frecuentes = [freq for word, freq in frecuencia_total_list[:5]]\n",
        "\n",
        "# Print the results\n",
        "print(\"Palabras más frecuentes:\")\n",
        "for palabra, frecuencia in zip(palabras_mas_frecuentes, frecuencias_mas_frecuentes):\n",
        "    print(f\"{palabra}: {frecuencia}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEs2QraPwxag",
        "outputId": "4926321d-3a85-4e67-ec29-09bcad369b8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras más frecuentes:\n",
            "/: 8\n",
            "e: 5\n",
            "s: 5\n",
            "o: 4\n",
            "a: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de las Partes del Discurso (POS)**"
      ],
      "metadata": {
        "id": "aU25om3kowBY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Distribución de Longitud de Palabras**"
      ],
      "metadata": {
        "id": "ImJPC5Ymo5X4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distribución de la Frecuencia de Palabras Únicas**"
      ],
      "metadata": {
        "id": "N-AUpsbTqvVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de N-gramas**"
      ],
      "metadata": {
        "id": "kfSRoRLrq7ls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis de la Diversidad Léxica**"
      ],
      "metadata": {
        "id": "h9bzRbdnrB-w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de Palabras con Word Cloud**"
      ],
      "metadata": {
        "id": "0rx4_YF-rGhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusiones del Analisis exploratorio**"
      ],
      "metadata": {
        "id": "SDy994LxrLSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocesamiento del Texto**"
      ],
      "metadata": {
        "id": "DwXACyM-rUZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenización:\n",
        "\n",
        "Tokenizar el texto en palabras, frases o párrafos según sea necesario.\n",
        "\n",
        "Limpieza del Texto:\n",
        "\n",
        "Convertir a minusculas, eliminar caracteres no deseados, stopwords, lematización, y stemming.\n",
        "\n",
        "Análisis Léxico y Morfológico:\n",
        "\n",
        "Identificación de partes del discurso (POS tagging) y análisis morfológico."
      ],
      "metadata": {
        "id": "uzxU7IE7rYte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Sintáctico**"
      ],
      "metadata": {
        "id": "TAhIjPnlrlY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing:\n",
        "\n",
        "Construir árboles sintácticos para las oraciones en el corpus.\n",
        "\n",
        "Dependencia Sintáctica:\n",
        "\n",
        "Análisis de dependencias para entender las relaciones gramaticales entre palabras."
      ],
      "metadata": {
        "id": "ofAV8fVdrxtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Semántico**"
      ],
      "metadata": {
        "id": "C_zqx-0pr3_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coreferencia:\n",
        "\n",
        "Resolver las referencias cruzadas en el texto para entender a qué se refieren los pronombres y otras expresiones.\n",
        "\n",
        "Análisis de Coherencia:\n",
        "\n",
        "Evaluar la coherencia y cohesión del discurso."
      ],
      "metadata": {
        "id": "gZSiRXc0sOm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Análisis Pragmático**"
      ],
      "metadata": {
        "id": "hH99SSBzsXc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Análisis de Sentimientos:\n",
        "\n",
        "Determinar el tono y las emociones expresadas en el texto.\n",
        "\n",
        "Detección de Intenciones:\n",
        "\n",
        "Identificar las intenciones detrás de las expresiones del texto."
      ],
      "metadata": {
        "id": "hUWts_4usgo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Codificación de texto a vectores**"
      ],
      "metadata": {
        "id": "SFh5txD0slcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of word\n",
        "\n",
        "Tf-IDF\n",
        "\n",
        "Word Embendings"
      ],
      "metadata": {
        "id": "imQS1D29sqfJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection**"
      ],
      "metadata": {
        "id": "J1QMfHxvsusd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selección de variable objetivo y variables independientes**"
      ],
      "metadata": {
        "id": "ffwi-JVpsz4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificación de Texto**\n",
        "\n",
        "X (Entrada): Texto del documento (puede ser una oración, párrafo, o artículo completo).\n",
        "\n",
        "Y (Salida): Etiqueta de la categoría del texto (por ejemplo, \"positivo\" o \"negativo\" para análisis de sentimientos, o categorías como \"deportes\", \"política\", \"tecnología\" para clasificación de noticias).\n",
        "\n",
        "**Análisis de Sentimientos**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Sentimiento asociado (por ejemplo, \"positivo\", \"negativo\" o \"neutral\").\n",
        "\n",
        "**Reconocimiento de Entidades Nombradas (NER)**\n",
        "\n",
        "X (Entrada): Texto del documento.\n",
        "\n",
        "Y (Salida): Entidades reconocidas y sus categorías (por ejemplo, \"PERSONA\", \"ORGANIZACIÓN\", \"LUGAR\").\n",
        "\n",
        "**Traducción Automática**\n",
        "\n",
        "X (Entrada): Texto en el idioma de origen.\n",
        "\n",
        "Y (Salida): Texto traducido al idioma de destino.\n",
        "\n",
        "**Resumen de Texto**\n",
        "\n",
        "X (Entrada): Texto completo del documento.\n",
        "\n",
        "Y (Salida): Resumen del documento.\n",
        "\n",
        "Generación de Texto **texto en negrita**\n",
        "\n",
        "X (Entrada): Prompt o inicio de una frase o párrafo.\n",
        "\n",
        "Y (Salida): Texto generado continuando el prompt.\n",
        "\n",
        "**Modelos de Lenguaje (Language Modeling)**\n",
        "\n",
        "X (Entrada): Una secuencia de palabras o caracteres.\n",
        "\n",
        "Y (Salida): La siguiente palabra o carácter en la secuencia."
      ],
      "metadata": {
        "id": "TIAOc0vus5HG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos**"
      ],
      "metadata": {
        "id": "udkFVq-MttGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Librerias necesarias para implementar los modelos**"
      ],
      "metadata": {
        "id": "1RjM87TLtvuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**División de datos en conjuntos de entrenamiento y prueba**"
      ],
      "metadata": {
        "id": "xqm8o1akt4Jz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicción con conjunto de prueba**"
      ],
      "metadata": {
        "id": "kwL8NaD5t9VX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del rendimiento del modelo**"
      ],
      "metadata": {
        "id": "DQYju67PuCrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre el modelado y las metricas**"
      ],
      "metadata": {
        "id": "UmqwS1z9uLo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optimización de modelos**"
      ],
      "metadata": {
        "id": "7v8OyPWWuRDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión sobre la optimización**"
      ],
      "metadata": {
        "id": "IcA9t9msudUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "stRic8RiufKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelos utilizando Redes Neuronales**"
      ],
      "metadata": {
        "id": "tszqOKGEuvDx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definición del problema**"
      ],
      "metadata": {
        "id": "y-mf6nIku09T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determinar qué quieres lograr con la red neuronal (clasificación, regresión, predicción de series temporales, etc.).\n",
        "\n",
        "Identificar las variables de entrada (features) y la salida esperada (labels o targets).\n",
        "\n",
        "Según el problema, seleccionar el tipo adecuado: Perceptrón Multicapa (MLP), CNN, RNN, etc."
      ],
      "metadata": {
        "id": "eD_WpQ39u8x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diseño de la red neuronal**"
      ],
      "metadata": {
        "id": "lmAiNXTIvBG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estructura:\n",
        "\n",
        "Seleccionar la cantidad de capas (profundidad) y neuronas por capa (ancho). Escoger la función de activación para cada capa (ReLU, Sigmoid, Softmax, etc.).\n",
        "\n",
        "Conexiones:\n",
        "\n",
        "Definir cómo se conectan las capas (densa, convolucional, recurrente, etc.). Pérdida y optimización:\n",
        "\n",
        "Elegir una función de pérdida (Cross-Entropy, MSE, etc.) según el problema. Seleccionar un optimizador (SGD, Adam, etc.)."
      ],
      "metadata": {
        "id": "tUB8XT4VvRoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento del modelo**"
      ],
      "metadata": {
        "id": "kPJVQ_KwvV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inicialización: Iniciar pesos y sesgos de manera adecuada (aleatorio, Xavier, He, etc.).\n",
        "\n",
        "Propagación hacia adelante (Forward pass): Calcular las predicciones. Cálculo de pérdida: Comparar las predicciones con las etiquetas esperadas.\n",
        "\n",
        "Propagación hacia atrás (Backpropagation): Ajustar los pesos utilizando el gradiente descendente.\n",
        "\n",
        "Iteraciones (Epochs): Repetir los pasos anteriores hasta alcanzar un buen desempeño o una condición de parada."
      ],
      "metadata": {
        "id": "LyH3PwU1va2a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "26t6OQABvkwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validación y ajuste**"
      ],
      "metadata": {
        "id": "GYJFPk9Yve54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluación del modelo**"
      ],
      "metadata": {
        "id": "PTpPrzwNvl5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualización de resultados**"
      ],
      "metadata": {
        "id": "qmzFsz2kvp5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones Finales**"
      ],
      "metadata": {
        "id": "NgivsfisvuAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tipos de modelo de Redes Neuronales**"
      ],
      "metadata": {
        "id": "PR8pb3vZv-Ve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo CNN es bueno para datos espaciales (como imágenes).\n",
        "\n",
        "El modelo RNN es ideal para datos secuenciales o dependientes del tiempo."
      ],
      "metadata": {
        "id": "g0n04s_3wEG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo CNN Típico**"
      ],
      "metadata": {
        "id": "riexdohYwJfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa convolucional:**\n",
        "\n",
        "Detecta patrones básicos en las imágenes, como bordes o texturas.\n",
        "\n",
        "**MaxPooling:**\n",
        "\n",
        "Reduce la dimensionalidad espacial para simplificar el modelo.\n",
        "\n",
        "**Flatten:**\n",
        "\n",
        "Convierte la salida bidimensional en un vector para pasar a las capas densas.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Previene el sobreajuste al apagar algunas neuronas aleatoriamente.\n",
        "\n",
        "Capa densa (Activación Sigmoide o Softmax):\n",
        "\n",
        "Genera probabilidades de pertenencia para cada clase."
      ],
      "metadata": {
        "id": "uPdApTHcwN67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo RNN Típico**"
      ],
      "metadata": {
        "id": "pLd2MxCQwXc9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Capa Embedding:**\n",
        "\n",
        "Convierte palabras o categorías en vectores densos\n",
        "\n",
        "**Capa RNN, LSTM o GRU:**\n",
        "\n",
        "Captura dependencias temporales en las secuencias de datos.\n",
        "\n",
        "**Dropout:**\n",
        "\n",
        "Reduce el riesgo de sobreajuste.\n",
        "\n",
        "**Capas densas:**\n",
        "Procesan las características aprendidas para clasificar las secuencias."
      ],
      "metadata": {
        "id": "H7rkTKvSwcxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5wcN6dwzwm76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TfSmQYQ5vkJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z-elUerjs_au"
      }
    }
  ]
}